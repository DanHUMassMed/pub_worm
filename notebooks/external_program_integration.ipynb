{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External Programs Integated with Pub_worm\n",
    "\n",
    "* Look at external programs and use pub_worm to solve problems\n",
    "* Look at BioPythons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sytem level imports\n",
    "import sys\n",
    "import asyncio\n",
    "import json\n",
    "import inspect\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl.utils import get_column_letter\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Add pub_worm directory to the Python path\n",
    "sys.path.insert(0, \"/Users/dan/Code/Python/pub_worm\")\n",
    "\n",
    "from pub_worm.ncbi.entreze_api import EntrezAPI\n",
    "from pub_worm.biorxiv.biorxiv_api import biorxiv_recent_posts_filtered\n",
    "\n",
    "# Find where EntrezAPI is being load from\n",
    "module = inspect.getmodule(EntrezAPI)\n",
    "if hasattr(module, \"__file__\"):\n",
    "    file_path = module.__file__\n",
    "    print(\"EntrezAPI imported from:\", file_path)\n",
    "else:\n",
    "    print(\"Could not determine the file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio import Entrez\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "Entrez.api_key = api_key = os.environ.get('NCBI_API_KEY', None)\n",
    "Entrez.email = \"daniel.higgins@yahoo.com\"\n",
    "\n",
    "# Call BioPython get a list of Databases that are managed by NCBI Entrez\n",
    "stream = Entrez.einfo()\n",
    "result = stream.read()\n",
    "stream.close()\n",
    "\n",
    "soup = BeautifulSoup(result, \"xml\")\n",
    "db_name_tags = soup.find_all('DbName')\n",
    "db_names = [db_name_tag.get_text(strip=True) for db_name_tag in db_name_tags]\n",
    "print(db_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Bio Python Entrez to get additional details on the NCBI Databases\n",
    "# NOTE: This seems very slow??\n",
    "for db_name in db_names:\n",
    "    stream = Entrez.einfo(db=db_name)\n",
    "    record = Entrez.read(stream)\n",
    "    print(record)\n",
    "    print(\"=\"*40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a networkx graph \n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_legend_map(edge_labels):\n",
    "    legend_map={'start_xxx':0}\n",
    "    for value in list(edge_labels.values()):\n",
    "        if value not in legend_map:\n",
    "            max_val = max(list(legend_map.values()))\n",
    "            legend_map[value] = max_val+1\n",
    "    del legend_map['start_xxx']\n",
    "    return legend_map\n",
    "\n",
    "def plot_network_graph(df):\n",
    "\n",
    "    # Create a directed graph\n",
    "    G = nx.from_pandas_edgelist(df, 'source', 'target', edge_attr='edge', create_using=nx.Graph())\n",
    "\n",
    "    # Draw the network diagram with a larger figure size\n",
    "    plt.figure(figsize=(40, 20))  # Set the figure size to 12x8 inches\n",
    "    pos = nx.spring_layout(G)  # positions for all nodes\n",
    "    nx.draw(G, pos, with_labels=True, node_size=4000, node_color='skyblue', font_size=9, font_color='black', edge_color='gray', linewidths=0.5, arrows=False)\n",
    "\n",
    "    # Add edge labels\n",
    "    edge_labels = nx.get_edge_attributes(G, 'edge')\n",
    "    print(type(edge_labels))\n",
    "    # print(edge_labels)\n",
    "    # legend_map = build_legend_map(edge_labels)\n",
    "    # edge_labels_mapped = {}\n",
    "    # for edge_label in edge_labels:\n",
    "    #     edge_label_value = edge_labels[edge_label]\n",
    "    #     map_value = legend_map[edge_label_value]\n",
    "    #     edge_labels_mapped[edge_label]=map_value\n",
    "    # print(legend_map)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "\n",
    "    # y_pos = 0.0\n",
    "    # for key, value in legend_map.items():\n",
    "    #     plt.text(0.0, y_pos, f\"{value} = {key}\", fontsize=12)\n",
    "    #     y_pos -= 0.1  # Adjust the y position for the next text\n",
    "\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('output/test.csv')\n",
    "edge_names = [\n",
    "    'Phenols',\n",
    "    'Lactones',\n",
    "    'Organic carbonic acids and derivatives',\n",
    "    '6-oxopurines'\n",
    "]\n",
    "selected_rows = df[df['edge'].isin(edge_names)]\n",
    "selected_rows.to_csv(\"output/test1.csv\")\n",
    "plot_network_graph(selected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "slim_metabolite_df = pd.read_csv(\"output/slim_metabolite.csv\")\n",
    "slim_metabolite_t_df = slim_metabolite_df.T\n",
    "slim_metabolite_t_df.to_csv(\"output/slim_motabolite_t.csv\",index_label='motabolite')\n",
    "slim_metabolite_t_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Iterate over each row and create a list of dictionaries\n",
    "list_of_dicts = []\n",
    "for idx, row in slim_metabolite_t_df.iterrows():\n",
    "    cleaned_row = row.dropna().tolist()\n",
    "    row_dict = {str(idx): cleaned_row}\n",
    "    list_of_dicts.append(row_dict)\n",
    "\n",
    "# Print the list of dictionaries\n",
    "print(list_of_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_to_ignore = ['Chemical entities', 'Hydrocarbon derivatives', 'Organic compounds', 'Organic oxygen compounds', \n",
    "                   'Organooxygen compounds', 'Organic oxides', 'Organic acids and derivatives', 'Organonitrogen compounds', \n",
    "                   'Organopnictogen compounds', 'Organic nitrogen compounds']\n",
    "def shares_edge(source_edge, target):\n",
    "    ret_val=False\n",
    "    target_nm = list(target.keys())[0]\n",
    "    target_edges = target[target_nm]\n",
    "    if source_edge in target_edges:\n",
    "        ret_val = True\n",
    "    return ret_val\n",
    "\n",
    "\n",
    "graph_list = []\n",
    "for index, list_of_dict in enumerate(list_of_dicts):\n",
    "    source_nm = list(list_of_dict.keys())[0]\n",
    "    source_edges  = list_of_dict[source_nm]\n",
    "    #print(f\"{source_nm=} {source_edges=}\")\n",
    "    targets = list_of_dicts[index+1:]\n",
    "    for source_edge in source_edges:\n",
    "        if source_edge not in edges_to_ignore:\n",
    "            for target in targets:\n",
    "                target_nm = list(target.keys())[0]\n",
    "                if shares_edge(source_edge, target):\n",
    "                    graph_list.append({'source':source_nm,'target':target_nm,'edge':source_edge})\n",
    "\n",
    "graph_df = pd.DataFrame(graph_list)\n",
    "graph_df.head()\n",
    "graph_df.to_csv(\"output/test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get full text for Journal Articles related to a Wormbase Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pub_worm.ncbi.entreze_api import EntrezAPI\n",
    "from pub_worm.wormbase.wormbase_api import WormbaseAPI\n",
    "\n",
    "async def get_pmid_for_wbpid(reference):\n",
    "    wormbase_api = WormbaseAPI(\"field\", \"paper\", \"pmid\")\n",
    "    pmid = wormbase_api.get_wormbase_data(reference['wbp_id'])\n",
    "    return {**reference, **pmid}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_unique_wbp_ids(references, unique_references):\n",
    "    \"\"\"\n",
    "    Updates the set of unique wbp_ids with new IDs from the list of references\n",
    "    and returns the list of references that have not been seen before.\n",
    "    \"\"\"\n",
    "    new_references = []\n",
    "    \n",
    "    # Iterate through the references\n",
    "    wbp_ids = set(unique_references.keys())\n",
    "    for ref in references:\n",
    "        wbp_id = ref[\"wbp_id\"]\n",
    "        # If the wbp_id is not in the set, add it to the set and add the reference to the new list\n",
    "        if wbp_id not in wbp_ids:\n",
    "            #print(ref)\n",
    "            unique_references[wbp_id]={'title':ref['wbp_title'], 'abstract':ref.get('wbp_abstract',\"\"),'pmcid':0}\n",
    "            new_references.append(ref)\n",
    "    \n",
    "    return new_references, unique_references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def get_references_wbid(wormbase_id, unique_references):\n",
    "    results_file_nm = f\"./output/{wormbase_id}.json\"\n",
    "    print(f\"Processing {wormbase_id}\")\n",
    "    wormbase_api = WormbaseAPI(\"field\", \"gene\", \"references\")\n",
    "    \n",
    "    # 1. Get all the references for the given wormbase_id\n",
    "    wormbase_data = wormbase_api.get_wormbase_data(wormbase_id)\n",
    "    \n",
    "    # 2a. collect only Journal articles\n",
    "    if isinstance(wormbase_data['references_list'], dict):\n",
    "        references = [wormbase_data['references_list']] # Make sure we have a list\n",
    "    else:\n",
    "        references = wormbase_data['references_list']        \n",
    "    journal_articles = [ref for ref in references if ref['wbp_type'] == 'Journal article' ]\n",
    "    \n",
    "    # 2b. collect only articles that we have not seen\n",
    "    journal_articles, unique_references = update_unique_wbp_ids(journal_articles, unique_references)\n",
    "\n",
    "    # 3a. Get the associated Pubmed Ids\n",
    "    pmid_for_wbpid_list = await asyncio.gather(*[get_pmid_for_wbpid(ref) for ref in journal_articles])\n",
    "    # 3b. Create a lookup table for pmid to bwpid\n",
    "    pmid_to_bwpid_lookup = {pmid_for_wbpid['pm_id']: pmid_for_wbpid['wbp_id'] for pmid_for_wbpid in pmid_for_wbpid_list}\n",
    "    # 3c. Add PubMed Ids to the unique_references\n",
    "    for pmid_for_wbpid in pmid_for_wbpid_list:\n",
    "        wbp_id = pmid_for_wbpid['wbp_id']\n",
    "        unique_references[wbp_id]['pmid']= pmid_for_wbpid['pm_id']\n",
    "\n",
    "    # 4. Extract the pubmed ids into a list\n",
    "    pmid_list = [pmid_for_wbpid['pm_id'] for pmid_for_wbpid in pmid_for_wbpid_list]\n",
    "    \n",
    "    # 5. Post the list to ncbi entrez\n",
    "    ncbi_api = EntrezAPI()\n",
    "    entreze_epost_result = ncbi_api.entreze_epost(pmid_list)\n",
    "    \n",
    "    # 6. Fetch the full articles\n",
    "    if 'WebEnv' in entreze_epost_result:\n",
    "        # 6a. Link pubmed ids to pmcids\n",
    "        elink_result = ncbi_api.entreze_elink_pmid_to_pmcid(entreze_epost_result)\n",
    "        params= {'db': 'pmc'}\n",
    "        # 6b. post the pmcids\n",
    "        epost_result = ncbi_api.entreze_epost(elink_result, params)\n",
    "        # 6c. Fetch the articles based on the pmcids\n",
    "        efetch_result = ncbi_api.entreze_efetch(epost_result)\n",
    "        \n",
    "        # 6d. Write the content of the paper to a file\n",
    "        for article in efetch_result['articles']:\n",
    "            content = f\"Title:{article['title']}\\n\"\n",
    "            content += f\"Abstract: {article['abstract']}\\n\"\n",
    "            content += f\"Content: {article['body']}\\n\"\n",
    "            file_nm = f\"./output/PMC{article['pmcid']}.txt\"\n",
    "            with open(file_nm, 'w') as file:\n",
    "                file.write(content)\n",
    "            wbp_id = pmid_to_bwpid_lookup[article['pmid']]\n",
    "            unique_references[wbp_id]['pmcid'] = article['pmcid']\n",
    "            \n",
    "    \n",
    "    # #print(f\"unique_references {len(unique_references)}\")\n",
    "    #print(json.dumps(journal_articles, indent=4))\n",
    "    return unique_references\n",
    "\n",
    "def pmc_not_found(unique_references):\n",
    "    for unique_reference in unique_references.values():\n",
    "        if unique_reference['pmcid'] == 0:\n",
    "            content = f\"Title:{unique_reference['title']}\\n\"\n",
    "            content += f\"Abstract: {unique_reference['abstract']}\\n\"\n",
    "            content += f\"Content: \\n\"\n",
    "            file_nm = f\"./output/PM{unique_reference['pmid']}.txt\"\n",
    "            with open(file_nm, 'w') as file:\n",
    "                file.write(content)\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gene_set = [\"WBGene00008850\",\"WBGene00001463\"]\n",
    "gene_set = [\"WBGene00016064\", \"WBGene00001463\", \"WBGene00001452\", \"WBGene00002048\", \"WBGene00003750\", \"WBGene00006575\", \"WBGene00006783\",\n",
    "            \"WBGene00019327\", \"WBGene00008850\", \"WBGene00019967\", \"WBGene00001452\", \"WBGene00001752\", \"WBGene00002048\", \"WBGene00003640\", \n",
    "            \"WBGene00007867\", \"WBGene00007875\", \"WBGene00008010\", \"WBGene00008584\", \"WBGene00008681\", \"WBGene00009429\", \"WBGene00016596\", \n",
    "            \"WBGene00019619\", \"WBGene00010290\", \"WBGene00000399\", \"WBGene00001430\", \"WBGene00010493\", \"WBGene00004512\", \"WBGene00004513\", \n",
    "            \"WBGene00004622\"]\n",
    "unique_references = {}\n",
    "for wormbase_id in gene_set:\n",
    "    unique_references = await get_references_wbid(wormbase_id, unique_references)\n",
    "\n",
    "pmc_not_found(unique_references)\n",
    "\n",
    "print(json.dumps(unique_references, indent=4))\n",
    "with open(\"./output/unique_references.json\", 'w') as file:\n",
    "    json.dump(unique_references, file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"35036864\", \"32292107\"]\n",
    "\n",
    "ncbi_api = EntrezAPI()\n",
    "entreze_epost_result = ncbi_api.entreze_epost(data)\n",
    "\n",
    "if 'WebEnv' in entreze_epost_result:\n",
    "    elink_result = ncbi_api.entreze_elink_pmid_to_pmcid(entreze_epost_result)\n",
    "    params= {'db': 'pmc'}\n",
    "    epost_result = ncbi_api.entreze_epost(elink_result, params)\n",
    "    efetch_result = ncbi_api.entreze_efetch(epost_result)\n",
    "    pretty_data = json.dumps(efetch_result, indent=4)\n",
    "    print(pretty_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = biorxiv_recent_posts_filtered()\n",
    "for article in articles:\n",
    "    print(f\"Archive Date: {article['date']}\")\n",
    "    print(f\"Title:{article['title']}\")\n",
    "    print(article['doi'])\n",
    "    print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# URL to fetch XML data\n",
    "url = \"https://connect.biorxiv.org/biorxiv_xml.php?subject=all\"\n",
    "\n",
    "# Function to fetch and parse the XML\n",
    "def fetch_biorxiv_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        raise Exception(f\"Failed to retrieve data: {response.status_code}\")\n",
    "        \n",
    "biorxiv_data = fetch_biorxiv_data(url)\n",
    "\n",
    "root = ET.fromstring(biorxiv_data)\n",
    "for item in root.findall('.//item'):\n",
    "     print(item)\n",
    "print(biorxiv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to fetch XML data\n",
    "url = \"https://connect.biorxiv.org/biorxiv_xml.php?subject=all\"\n",
    "\n",
    "# Keywords to search in titles and descriptions\n",
    "keywords = [\"caenorhabditis\", \"elegans\"]\n",
    "\n",
    "# Function to fetch the XML data\n",
    "def fetch_biorxiv_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        raise Exception(f\"Failed to retrieve data: {response.status_code}\")\n",
    "\n",
    "# Function to search for keywords in text\n",
    "def contains_keywords(text, keywords):\n",
    "    return any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "\n",
    "# Function to parse the XML using BeautifulSoup and extract relevant articles\n",
    "def parse_biorxiv_xml(xml_data, keywords):\n",
    "    soup = BeautifulSoup(xml_data, \"xml\")\n",
    "    articles = []\n",
    "\n",
    "    # Iterate over each <item> in the XML\n",
    "    for item in soup.find_all('item'):\n",
    "        title = item.find('title').get_text(strip=True)\n",
    "        description = item.find('description').get_text(strip=True)\n",
    "        dc_date = item.find('dc:date').get_text(strip=True)\n",
    "        dc_identifier = item.find('dc:identifier').get_text(strip=True)\n",
    "        \n",
    "        # Check if either the title or description contains any of the keywords\n",
    "        if contains_keywords(title, keywords) or contains_keywords(description, keywords):\n",
    "            # Append the article details as a dictionary\n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'date': dc_date,\n",
    "                'doi': f\"https://doi.org/{dc_identifier}\"\n",
    "            })\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Main function to fetch, parse, and filter data\n",
    "def main():\n",
    "    # Fetch the XML data from the URL\n",
    "    xml_data = fetch_biorxiv_data(url)\n",
    "    \n",
    "    # Parse the XML and search for relevant articles\n",
    "    articles = parse_biorxiv_xml(xml_data, keywords)\n",
    "    \n",
    "    # Print the JSON list of filtered articles\n",
    "    if articles:\n",
    "        print(articles)\n",
    "    else:\n",
    "        print(\"No articles found with the given keywords.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dan-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
